package parallelai.spyglass.hbase

import java.io.File
import scala.collection.JavaConverters._
import scala.collection.mutable.Buffer
import com.google.common.io.Files
import cascading.pipe.Pipe
import cascading.tuple.{ Tuple => CTuple }
import org.apache.hadoop.fs.Path
import org.apache.hadoop.util.ToolRunner
import org.apache.hadoop.hbase.util.Bytes
import com.twitter.scalding.{ Job, Args, Tsv, JobTest }
import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster
import org.apache.hadoop.hbase.{ HBaseConfiguration, MiniHBaseCluster, HTableDescriptor, HColumnDescriptor, TableExistsException }
import org.apache.hadoop.hbase.client.{ HBaseAdmin, HTable, Scan, Delete, Result }
import org.scalatest.{ FunSpec, BeforeAndAfterAll }

class LoadJob(args: Args) extends Job(args) {
  Tsv("input", ('fruit, 'comment))
    .then((pipe: Pipe) => new HBasePipeWrapper(pipe).toBytesWritable(('fruit, 'comment)))
    .write(new HBaseSource("test", "localhost:2181", ('fruit), List("cf"), List("comment")))
}

object SpyGlassSpec {
  def getLocalPath(fileString: String): Path = new Path((new File(fileString).getAbsolutePath()).toString())

  def createTable(conf: HBaseConfiguration, tableName: String, family: String): HTable = {
    val desc = new HTableDescriptor(Bytes.toBytes(tableName))
    desc.addFamily(new HColumnDescriptor(Bytes.toBytes(family)))
    try {
      new HBaseAdmin(conf).createTable(desc)
    } catch {
      case e: TableExistsException => 1
    }
    new HTable(conf, tableName)
  }

  def truncTable(table: HTable) {
    for(res <- table.getScanner(new Scan).asScala) table.delete(new Delete(res.getRow))
  }

  def toMap(result: Result, family: String): Map[String, String] =
    Map() ++ result.getFamilyMap(Bytes.toBytes(family)).asScala.iterator.map{ case (key, value) => (Bytes.toString(key), Bytes.toString(value)) }

  def toMaps(results: Array[Result], family: String): Map[String, Map[String, String]] =
    results.map{ result => (Bytes.toString(result.getRow), toMap(result, family)) }.toMap

  def noOp[A](buffer: Buffer[A]) {}
}

class SpyGlassSpec extends FunSpec with BeforeAndAfterAll {
  import SpyGlassSpec._
  import com.twitter.scalding.Dsl._

  val conf = new HBaseConfiguration
  val tmpDir = Files.createTempDir
  val zkCluster = new MiniZooKeeperCluster
  val clientPort = zkCluster.startup(tmpDir)
  conf.set("hbase.zookeeper.property.clientPort", clientPort.toString)
  val hbaseCluster = new MiniHBaseCluster(conf, 1)
  val htable = createTable(conf, "test", "cf")

  describe("An HBaseSource") {
    it("should be able to participate in a flow as a sink") {
      truncTable(htable)
      JobTest("parallelai.spyglass.hbase.LoadJob")
        .source(Tsv("input", ('fruit, 'comment)), List(("apple", "from long island"), ("pear", null.asInstanceOf[String])))
        .sink(new HBaseSource("test", "localhost:2181", ('fruit), List("cf"), List("comment")))(noOp[(String, String)])
        .run
        .finish
      println("!!!!!!!!???????????????? " + toMaps(htable.getScanner(Bytes.toBytes("cf")).next(1000), "cf"))
      assert(toMaps(htable.getScanner(Bytes.toBytes("cf")).next(1000), "cf") == Map(
        "apple" -> Map("comment" -> "from long island"),
        "pear" -> Map("comment" -> "")
      ))
    }
  }

  override def afterAll(configMap: Map[String, Any]) {
    hbaseCluster.shutdown
    hbaseCluster.join
  }

}

